{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Frame\n",
    "This notebook will load the filtered and cleaned signals and markers and turn it into a useful dataframe that contains the epochs of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperFunctions import *\n",
    "from constants import *\n",
    "from dataAnalysisFunctions import getSEM\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Files that work with this notebook: \n",
    "Anything in filtered*_data\n",
    "\n",
    "'''\n",
    "# Load the file and folder you need\n",
    "filename_foldername_dict_path = \"../data/most_currently_updated.pickle\"\n",
    "filename_foldername_dict = loadPickle(filename_foldername_dict_path)\n",
    "\n",
    "foldername=filename_foldername_dict[\"foldername\"]\n",
    "filename=filename_foldername_dict[\"filename\"]\n",
    "print(foldername, filename)\n",
    "\n",
    "incorporate_matlab_data = False\n",
    "if incorporate_matlab_data: \n",
    "    filtered_matlab_data_directory = \"../data/filtered_matlab_data/\"+foldername+\"/\"\n",
    "    filtered_matlab_data_path = filtered_matlab_data_directory+filename+\".pickle\"\n",
    "    filtered_matlab_data = loadPickle(filtered_matlab_data_path)\n",
    "    all_data = filtered_matlab_data\n",
    "    data_type=\"data_matlab_extended\"\n",
    "else :\n",
    "    filtered_cleaned_data_directory = \"../data/filtered_cleaned_data/\"+foldername+\"/\"\n",
    "    filtered_cleaned_data_path = filtered_cleaned_data_directory+filename+\".pickle\"\n",
    "    filtered_cleaned_data = loadPickle(filtered_cleaned_data_path)\n",
    "    all_data = filtered_cleaned_data\n",
    "    data_type=\"data_extended\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sections\n",
    "numSections = getNumSections(filtered_cleaned_data)\n",
    "article_sections = list()\n",
    "for i in range(numSections) : \n",
    "    article_sections.append(getArticleSectionData(\"response\", i, filtered_cleaned_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Meme shown data and times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get meme shown data\n",
    "meme_shown_string = getLabelMarkerString(\"memeShown\")\n",
    "\n",
    "meme_is_shown_data, a, b = getLabelBoundSingleLabelData(\"memeShown\", \"memeHidden\", filtered_cleaned_data, go_backward=False)\n",
    "if len(meme_is_shown_data) == 0: \n",
    "    articles_with_memes = list()\n",
    "    articles_without_memes = list() \n",
    "    for section in article_sections:\n",
    "        if meme_shown_string in section[StreamType.MARKER.value][StreamType.DATA.value] :\n",
    "            articles_with_memes.append(section)\n",
    "        else : \n",
    "            articles_without_memes.append(section)\n",
    "    meme_is_shown_data = articles_with_memes\n",
    "    \n",
    "# Determine the time frames in which the memes are shown\n",
    "meme_shown_times = list() \n",
    "for data in meme_is_shown_data:\n",
    "    start_time = data[StreamType.MARKER.value][StreamType.TIME.value][0]\n",
    "    end_time = data[StreamType.MARKER.value][StreamType.TIME.value][-1]\n",
    "    meme_shown_times.append((start_time, end_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Eye Looking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TWEAK these numbers to see what valeus would work best when inputted into the function\n",
    "eye_looking_percent_data_directory = \"../data/eye_looking_percent/\"+foldername+\"/\"\n",
    "eye_looking_percent_data_path = eye_looking_percent_data_directory+filename+\".pickle\"\n",
    "ensureDirExists(eye_looking_percent_data_directory)\n",
    "\n",
    "try :\n",
    "    percent_distance = loadPickle(eye_looking_percent_data_path)\n",
    "    percent_distance_from_center_y = percent_distance[\"percent_distance_y\"]\n",
    "    percent_distance_from_center = percent_distance[\"percent_distance_x\"]\n",
    "    print(percent_distance)\n",
    "    \n",
    "except: \n",
    "    print(\"Tweak the percent distance from center values in this cell\") \n",
    "    percent_distance_from_center_y = 75/10\n",
    "    percent_distance_from_center = 7/10\n",
    "    percent_distance = {\"percent_distance_x\":percent_distance_from_center, \"percent_distance_y\":percent_distance_from_center_y}\n",
    "    print(percent_distance)\n",
    "    \n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_x_data=filtered_cleaned_data[StreamType.EYENORM.value][StreamType.DATA.value][:,0]\n",
    "eye_y_data=filtered_cleaned_data[StreamType.EYENORM.value][StreamType.DATA.value][:,1]\n",
    "eye_time_data=filtered_cleaned_data[StreamType.EYENORM.value][StreamType.TIME.value]\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.plot(eye_time_data[:], eye_y_data[:] + 3, label=\"raw eye y data\")\n",
    "plt.plot(eye_time_data[:], eye_x_data[:] + 1, label=\"raw eye x data\")\n",
    "\n",
    "# Plot eye movement predictions\n",
    "avg_calibrated_x, avg_calibrated_y = getAvgCalibratedXandY(filtered_cleaned_data)\n",
    "looking_right = getLookingRightTimes(filtered_cleaned_data, avg_calibrated_x, avg_calibrated_y, percent_distance_from_center)\n",
    "looking_up = getLookingUpTimes(filtered_cleaned_data, avg_calibrated_x, avg_calibrated_y, percent_distance_from_center_y)\n",
    "\n",
    "plt.plot(eye_time_data[:], np.array(looking_up)+2, label=\"looking up\")\n",
    "plt.plot(eye_time_data[:], looking_right, label=\"looking right\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"The timepoints in which several are representative of this behavior\")\n",
    "\n",
    "\n",
    "# Plot meme shown sections\n",
    "meme_shown_data, a, b = getLabelBoundSingleLabelData(\"memeShown\", \"memeHidden\", filtered_cleaned_data, go_backward=False)\n",
    "for i in range(len(meme_shown_data)): \n",
    "    eye_x_data=meme_shown_data[i][StreamType.EYENORM.value][StreamType.DATA.value][:,0]\n",
    "    eye_y_data=meme_shown_data[i][StreamType.EYENORM.value][StreamType.DATA.value][:,1]\n",
    "    eye_time_data=meme_shown_data[i][StreamType.EYENORM.value][StreamType.TIME.value]\n",
    "\n",
    "    if i == 0:\n",
    "        plt.plot(eye_time_data, eye_x_data + 1, color=\"r\", label=\"meme shown time\")\n",
    "    else : \n",
    "        plt.plot(eye_time_data, eye_x_data + 1, color=\"r\")\n",
    "    plt.plot(eye_time_data, eye_y_data + 3, color=\"r\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_time_data=filtered_cleaned_data[StreamType.EYENORM.value][StreamType.TIME.value]\n",
    "\n",
    "\n",
    "filtered_cleaned_data[StreamType.LOOKING_UP.value] = dict()\n",
    "filtered_cleaned_data[StreamType.LOOKING_UP.value][StreamType.DATA.value] = np.array(looking_up)\n",
    "filtered_cleaned_data[StreamType.LOOKING_UP.value][StreamType.TIME.value] = np.array(eye_time_data)\n",
    "\n",
    "filtered_cleaned_data[StreamType.LOOKING_RIGHT.value] = dict()\n",
    "filtered_cleaned_data[StreamType.LOOKING_RIGHT.value][StreamType.DATA.value] = np.array(looking_right)\n",
    "filtered_cleaned_data[StreamType.LOOKING_RIGHT.value][StreamType.TIME.value] = np.array(eye_time_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(filtered_cleaned_data[StreamType.LOOKING_UP.value][StreamType.TIME.value][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the percent distances used\n",
    "print(percent_distance)\n",
    "ensureDirExists(eye_looking_percent_data_directory)\n",
    "writeToPickle(percent_distance, eye_looking_percent_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_marker_string = getLabelMarkerString(\"newWord\")\n",
    "end_marker_string = getLabelMarkerString(\"endWord\")\n",
    "target_word_string = getSingleLabelMarkerString(\"targetWord\")\n",
    "space_pressed_string = getSingleLabelMarkerString(\"spacePressed\")\n",
    "response_start_string = getSectionMarkerString(\"response\")[0] # Get the 0th element to get the start string\n",
    "space_pressed_string = getLabelMarkerString(\"spacePressed\")\n",
    "letters_shown_string = getLabelMarkerString(\"lettersShown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construct data frame \n",
    "\n",
    "# This version doesn't work because we need to know when the neew article starts. \n",
    "# new_word_data, a, t_data = getLabelBoundSingleLabelData(\"newWord\", \"endWord\", filtered_cleaned_data, go_backward=False)\n",
    "# points = [getTotalPoints(data) for data in new_word_data] \n",
    "\n",
    "EEG_MEAN_THRESHOLD = 200\n",
    "new_word_data, a, t_data = getTimeBoundSingleLabelData(\"newWord\", filtered_cleaned_data, time_before=0.5, time_after=1.7)\n",
    "\n",
    "points = [getTotalPoints(data) for data in new_word_data] \n",
    "\n",
    "data_info = dict() \n",
    "data_info[\"data_index\"] = list() \n",
    "data_info[\"data\"] = list() \n",
    "data_info[\"data_extended\"] = list()\n",
    "\n",
    "if incorporate_matlab_data:\n",
    "    new_word_matlab_data, am, tm_data = getTimeBoundSingleLabelData(\"newWord\", filtered_matlab_data, time_before=0.5, time_after=1.7)\n",
    "    data_info[\"data_matlab\"] = list()\n",
    "    data_info[\"data_matlab_extended\"] = list()\n",
    "    \n",
    "\n",
    "data_info[\"is_target\"] = list() \n",
    "data_info[\"is_pressed\"] = list()\n",
    "data_info[\"has_letters_shown\"] = list() \n",
    "data_info[\"has_meme\"] = list() \n",
    "data_info[\"time\"] = list() \n",
    "data_info[\"points\"] = list() \n",
    "data_info[\"reaction_time\"] = list() \n",
    "data_info[\"section_number\"] = list()\n",
    "data_info[\"looking_up\"] = list()\n",
    "data_info[\"looking_right\"] = list()\n",
    "data_info[\"trial_time\"] = list()\n",
    "data_info[\"word_length\"] = list()\n",
    "\n",
    "eeg_avg_val = list()\n",
    "included_val = list()\n",
    "\n",
    "section_number = 0\n",
    "\n",
    "end_times = list()\n",
    "for i, point in enumerate(points):\n",
    "    if(response_start_string in new_word_data[i][StreamType.MARKER.value][StreamType.DATA.value]) : \n",
    "        section_number += 1\n",
    "    \n",
    "    # Find this word's start and end indexes \n",
    "    marker_indexes = getMarkerIndexes(new_word_data[i])\n",
    "    start_word_index = marker_indexes[start_marker_string][0]\n",
    "    end_word_index = marker_indexes[end_marker_string][-1]\n",
    "    end_time = float(new_word_data[i][StreamType.MARKER.value][StreamType.TIME.value][end_word_index])\n",
    "    end_times.append(end_time)\n",
    "    # Determine if meme was shown\n",
    "    has_meme = False\n",
    "    for meme_shown_interval in meme_shown_times: \n",
    "        if (end_time > meme_shown_interval[0] and end_time < meme_shown_interval[1]) :\n",
    "            has_meme = True\n",
    "            break\n",
    "\n",
    "    # Determine if word is a target word\n",
    "    is_target = False\n",
    "    if target_word_string in new_word_data[i][StreamType.MARKER.value][StreamType.DATA.value][start_word_index:end_word_index] : \n",
    "        is_target = True\n",
    "    \n",
    "    # Determine if word was pressed on \n",
    "    is_pressed = False\n",
    "    if space_pressed_string in new_word_data[i][StreamType.MARKER.value][StreamType.DATA.value][start_word_index:end_word_index] : \n",
    "        is_pressed = True\n",
    "        \n",
    "    # Determine if word had letters shown\n",
    "    has_letters_shown = False\n",
    "    if letters_shown_string in new_word_data[i][StreamType.MARKER.value][StreamType.DATA.value][start_word_index:end_word_index] : \n",
    "        has_letters_shown = True\n",
    "            \n",
    "    \n",
    "    # Get reaction time\n",
    "    data_list = getTrials(new_word_data[i])\n",
    "    this_data = data_list[0]\n",
    "    reaction_time = getReactionTime(this_data)\n",
    "    trial_time = getTrialLength(this_data)\n",
    "    \n",
    "    # Get this trial's data\n",
    "    new_data, _, _ = getLabelBoundSingleLabelData(\"newWord\", \"endWord\", new_word_data[i], go_backward=False)\n",
    "    new_data = new_data[0]\n",
    "    \n",
    "    # Check if trial is an outlier\n",
    "    eeg_avg_abs = np.abs(np.mean(getEEGFromDataList_AvgLeftRight([new_data])))\n",
    "    \n",
    "    eeg_avg_val.append(eeg_avg_abs)\n",
    "    \n",
    "    # Reject if too outliery\n",
    "    if eeg_avg_abs > EEG_MEAN_THRESHOLD:\n",
    "        print(\"rejecting data\", i, \"from article\", section_number)\n",
    "        included_val.append(0)\n",
    "        continue\n",
    "        \n",
    "    word_length = getWordLength(new_data)\n",
    "    \n",
    "    included_val.append(eeg_avg_abs)\n",
    "    data_info[\"data_index\"].append(i)\n",
    "    \n",
    "    data_info[\"data\"].append(new_data)\n",
    "    data_info[\"data_extended\"].append(new_word_data[i])\n",
    "    \n",
    "    if incorporate_matlab_data:\n",
    "        new_matlab_data, _, _ = getLabelBoundSingleLabelData(\"newWord\", \"endWord\", new_word_matlab_data[i], go_backward=False)\n",
    "        new_matlab_data = new_matlab_data[0]\n",
    "        data_info[\"data_matlab\"].append(new_matlab_data)\n",
    "        data_info[\"data_matlab_extended\"].append(new_word_matlab_data[i])\n",
    "    \n",
    "    data_info[\"is_target\"].append(is_target)\n",
    "    data_info[\"is_pressed\"].append(is_pressed)\n",
    "    data_info[\"has_letters_shown\"].append(has_letters_shown)\n",
    "    data_info[\"has_meme\"].append(has_meme)\n",
    "    data_info[\"time\"].append(t_data[i])\n",
    "    data_info[\"points\"].append(point)\n",
    "    data_info[\"section_number\"].append(section_number)\n",
    "    data_info[\"reaction_time\"].append(reaction_time)\n",
    "    if StreamType.LOOKING_UP.value in new_word_data[i].keys() :\n",
    "        data_info[\"looking_up\"].append(np.sum(new_word_data[i][StreamType.LOOKING_UP.value][StreamType.DATA.value]))\n",
    "        data_info[\"looking_right\"].append(np.sum(new_word_data[i][StreamType.LOOKING_RIGHT.value][StreamType.DATA.value]))\n",
    "    else :\n",
    "        data_info[\"looking_up\"].append(0)\n",
    "        data_info[\"looking_right\"].append(0)\n",
    "    data_info[\"trial_time\"].append(trial_time)\n",
    "    data_info[\"word_length\"].append(word_length)\n",
    "\n",
    "df = pd.DataFrame.from_dict(data_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if any of the EEG data was excluded\n",
    "plt.plot(np.abs(eeg_avg_val), label=\"all eeg data\")\n",
    "plt.plot(np.abs(included_val), label=\"included eeg data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cleaned_data[StreamType.MARKER.value][StreamType.DATA.value][370:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"calibration markers:\\n\\n\",filtered_cleaned_data[StreamType.MARKER.value][StreamType.DATA.value][:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display reaction times\n",
    "plt.plot(df[\"has_meme\"], label=\"has meme\")\n",
    "plt.plot(df[\"reaction_time\"], label=\"reaction times\")\n",
    "\n",
    "plt.xlabel(\"trial\")\n",
    "plt.ylabel(\"1 = 1 sec or has meme. 0 = no response or no meme.\")\n",
    "plt.title(\"reaction time vs meme shown\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"This shows if there's any correlation between showing the memes and a elongation of reaction time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_average_reaction_times = list() \n",
    "article_sem_reaction_times = list() \n",
    "article_num_target_words = list()\n",
    "article_num_total_words = list()\n",
    "for i in range(1,numSections+1) :\n",
    "    df_article = df[df[\"section_number\"] == i]\n",
    "    \n",
    "    reaction_times = df_article[df_article[\"reaction_time\"] > 0][\"reaction_time\"].values\n",
    "    sem_reaction_times = getSEM(reaction_times)\n",
    "    sum_reaction_times = sum(reaction_times)\n",
    "    num_targets = len(df_article[df_article[\"is_target\"] > 0])\n",
    "    num_words = len(df_article)\n",
    "    \n",
    "    article_average_reaction_times.append(sum_reaction_times / num_targets)\n",
    "    article_sem_reaction_times.append(sem_reaction_times)\n",
    "    article_num_target_words.append(num_targets)\n",
    "    article_num_total_words.append(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(list(range(1, numSections+1)), article_average_reaction_times, yerr=article_sem_reaction_times)\n",
    "plt.title(\"Reaction times\")\n",
    "plt.xlabel(\"article section #\")\n",
    "plt.ylabel(\"seconds (s)\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display eye looking times\n",
    "plt.plot(df[\"has_meme\"] * 8, label=\"has meme\")\n",
    "\n",
    "plt.plot(df[\"looking_up\"], label=\"looking up times\")\n",
    "plt.plot(df[\"looking_right\"], label=\"looking right times\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"trial\")\n",
    "plt.ylabel(\"num cycles looking up or right\")\n",
    "plt.title(\"when deviation of eye is greater than a certain amount\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"This shows when the subject may be looking right or up\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Article section dividers with target word times\n",
    "plt.plot(df[\"is_target\"], label=\"target word\")\n",
    "plt.plot([ 0 if (i > 0) and (df[\"section_number\"][i-1] != df[\"section_number\"][i]) else 1 for i, row in df.iterrows()], label=\"article change\")\n",
    "plt.title(\"Target Word times and the article changes\")\n",
    "plt.xlabel(\"trial\")\n",
    "plt.ylabel(\"1 = target word, 0 = not target word\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"This shows the distribution of target words shown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(list(range(1, numSections+1)), article_num_target_words)\n",
    "plt.title(\"num target words per article\")\n",
    "plt.ylabel(\"num words\")\n",
    "plt.xlabel(\"article section #\")\n",
    "plt.show()\n",
    "print(\"target words per section\", article_num_target_words)\n",
    "\n",
    "plt.bar(list(range(1, numSections+1)), np.array(article_num_target_words) / np.array(article_num_total_words))\n",
    "plt.title(\"% target words per article\")\n",
    "plt.ylabel(\"% target words\")\n",
    "plt.xlabel(\"article section #\")\n",
    "plt.show()\n",
    "print(\"% of target words per section\", np.array(article_num_target_words) / np.array(article_num_total_words))\n",
    "\n",
    "plt.bar(list(range(1, numSections+1)), article_num_total_words)\n",
    "plt.title(\"num words per article\")\n",
    "plt.ylabel(\"num words\")\n",
    "plt.xlabel(\"article section #\")\n",
    "plt.show()\n",
    "print(\"total words per section\", article_num_total_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfocused State visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfocused_state = list(((df[\"is_pressed\"] == False) & (df[\"is_target\"] == True)) | ((df[\"is_pressed\"] == True) & (df[\"is_target\"] == False)))\n",
    "\n",
    "plt.plot(df[\"trial_time\"].values, label=\"trial time\")\n",
    "plt.plot(list(range(len(unfocused_state))), [1 if state else 0.8 for state in unfocused_state], label=\"unfocus state\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Trial time and focus state\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df[\"looking_up\"].values, label=\"looking up\")\n",
    "plt.plot(df[\"looking_right\"].values, label=\"looking right\")\n",
    "plt.plot(df[\"has_meme\"] * 8, label=\"has meme\")\n",
    "\n",
    "\n",
    "plt.plot(list(range(len(unfocused_state))), [1 if state else 0 for state in unfocused_state], label=\"unfocus state\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Potential distraction precursors and focus state\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"avg word length:\", np.mean(df[\"word_length\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focused_word_lengths = df[[False if state else True for state in unfocused_state]][\"word_length\"].values\n",
    "unfocused_word_lengths = df[unfocused_state][\"word_length\"].values\n",
    "\n",
    "plt.bar([\"focused\", \"unfocused\"], [np.mean(focused_word_lengths), np.mean(unfocused_word_lengths)], yerr=[getSEM(focused_word_lengths), getSEM(unfocused_word_lengths)])\n",
    "plt.title(\"word lengths for focused and unfocused trials\")\n",
    "plt.show()\n",
    "\n",
    "print(\"If focused is significantly smaller, may be because word lengths are too long in unfocused states\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df[\"points\"].values, label=\"point for trial\")\n",
    "plt.plot(df[\"has_meme\"].values, label=\"has_meme\")\n",
    "plt.legend() \n",
    "plt.title(\"Meme vs points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot points\n",
    "for i, article_section in enumerate(article_sections):\n",
    "    data, time = getPointsAfterEachWord(article_section)\n",
    "    num_delta_points = getTotalPoints(article_section)\n",
    "    plt.plot(time[:], data[:], label=\"section \" + str(i) + \": \" + str(num_delta_points) + \" points\")\n",
    "plt.title(\"point values\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_directory = \"../data/dataframe/\"+foldername+\"/\"\n",
    "dataframe_path = dataframe_directory+filename+\".pickle\"\n",
    "\n",
    "ensureDirExists(dataframe_directory)\n",
    "\n",
    "writeToPickle(df, dataframe_path)\n",
    "print(\"Done!\")\n",
    "print(foldername, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
